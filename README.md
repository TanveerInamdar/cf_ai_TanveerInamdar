# cf_ai_bug_sherlock

**AI-powered debugging assistant built on Cloudflare Workers.**  
This project captures frontend bug reports and analyzes them with **Cloudflare Workers AI (Llama 3.1 8B)** to explain the cause and suggest a fix.

---

## ğŸ§  What It Does

When a user clicks **â€œReport Bugâ€**, the Worker:
1. Captures a simulated frontend error.
2. Sends it to Workers AI for analysis.
3. Displays an explanation and suggested fix right in the browser.
4. Stores the report in a KV Namespace for reference.

---

## ğŸ§© Architecture Overview

```
[ Browser ]
â”‚
â”‚ (1) "Report Bug" â†’ sends JSON payload
â–¼
[ Cloudflare Worker ]
â”‚
â”‚ (2) Calls Workers AI â†’ @cf/meta/llama-3.1-8b-instruct
â–¼
[ Workers AI Model ]
â”‚
â”‚ (3) Returns analysis + fix suggestion
â–¼
[ Worker ]
â”‚
â”‚ (4) Saves record â†’ KV Namespace (BUG_MEMORY)
â–¼
[ Browser ]
â”‚
â”‚ (5) Displays formatted "AI Analysis" box
```

---

## ğŸš€ Setup (LOCAL BUILD ONLY)

You do **not** need to deploy this project.  
It runs locally and temporarily uses Cloudflareâ€™s remote AI during testing.

### 1ï¸âƒ£ Clone and install
```bash
git clone https://github.com/<your-username>/cf_ai_bug_sherlock.git
cd cf_ai_bug_sherlock
npm install
```

### 2ï¸âƒ£ Configure `wrangler.toml`

Make sure it looks like this:

```toml
name = "cf_ai_bug_sherlock"
main = "src/worker.js"
compatibility_date = "2025-10-01"

[ai]
binding = "AI"

[[kv_namespaces]]
binding = "BUG_MEMORY"
id = "<YOUR_KV_NAMESPACE_ID>"
preview_id = "<YOUR_KV_PREVIEW_ID>"
```

The KV IDs can stay as placeholders for local testing.  
No secret keys or tokens are needed.

### 3ï¸âƒ£ Enable Workers AI (once)
Go to Cloudflare Dashboard â†’ **Workers AI**,  
and make sure Workers AI is enabled on your account.

### 4ï¸âƒ£ Run the project locally (in remote mode)
```bash
npx wrangler dev --remote
```

This launches your Worker on Cloudflareâ€™s edge temporarily so it can access the AI model.  
Wait until Wrangler shows:

```
[wrangler:inf] Ready on http://127.0.0.1:8787
```

### 5ï¸âƒ£ Test the demo

Visit:
```
http://127.0.0.1:8787/
```

Click â€œReport Bugâ€.  
Youâ€™ll see an **AI Analysis** box appear below the button with an explanation and suggested fix.

### 6ï¸âƒ£ Stop the server
Press `Ctrl + C` in the terminal to stop Wrangler.

---

## âš™ï¸ Configuration Details

| Component | Purpose |
|------------|----------|
| Workers AI Binding (`env.AI`) | Runs `@cf/meta/llama-3.1-8b-instruct` model |
| KV Namespace (`BUG_MEMORY`) | Stores recent bug reports |
| Compatibility Date | `2025-10-01` |
| Language | JavaScript (ES Modules) |

---

## ğŸ”’ Security Notes

No API keys, tokens, or credentials are included.  
`wrangler.toml` only contains resource IDs, which are safe to commit.  
The `.wrangler/` folder stores local config and should be ignored.

Add a small `.gitignore` file:

```
node_modules/
.wrangler/
dist/
```

---

## ğŸ§© Example Output

```
AI Analysis:
The error "TypeError: Cannot read properties of undefined" occurs when trying to
access a property on an undefined object. Ensure variables are initialized or add
a null check before accessing properties.
```

---

## ğŸ’¡ Future Improvements

- Add `/dashboard` route to view KV-stored bug reports.  
- Integrate with GitHub or Linear for automatic issue creation.  
- Extend LLM prompt chain for root-cause + code patch generation.

---

## ğŸ“„ License

MIT Â© 2025 Tanveer Siddharth Inamdar
